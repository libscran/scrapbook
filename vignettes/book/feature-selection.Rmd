---
output:
  html_document
bibliography: ref.bib
---

# Feature selection 

```{r, echo=FALSE}
knitr::opts_chunk$set(message=FALSE, warning=FALSE, error=FALSE)
```

## Motivation

We often use scRNA-seq data in exploratory analyses to characterize heterogeneity across the cell population.
Procedures like clustering and dimensionality reduction compare cells based on their gene expression profiles,
which involves aggregating per-gene differences into a single (dis)similarity metric between a pair of cells.
The choice of genes to use in this calculation has a major impact on the behavior of the metric and the performance of downstream methods.
We want to select genes that contain useful information about the biology of the system while removing genes that contain random noise.
In addition to improving signal, this reduces the size of the data to improve computational efficiency of later steps.

## Selecting highly variable genes

### Modelling the mean-variance trend

Our aim is to select the most highly variable genes (HVGs) based on their expression across the population.
This assumes that genuine biological differences will manifest as increased variation in the affected genes,
compared to other genes that are only affected by technical noise or a baseline level of "uninteresting" biological variation (e.g., from transcriptional bursting).
To demonstrate, we'll pull out the classic PBMC dataset from 10X Genomics [@zheng2017massively]:

```{r}
# Loading in raw data from the 10X output files.
library(DropletTestFiles)
raw.path.10x <- getTestFile("tenx-2.1.0-pbmc4k/1.0.0/filtered.tar.gz")
dir.path.10x <- file.path(tempdir(), "pbmc4k")
untar(raw.path.10x, exdir=dir.path.10x)

library(DropletUtils)
fname.10x <- file.path(dir.path.10x, "filtered_gene_bc_matrices/GRCh38")
sce.10x <- read10xCounts(fname.10x, col.names=TRUE)

# Applying our default QC with outlier-based thresholds.
library(scrapper)
is.mito.10x <- grepl("^MT-", rowData(sce.10x)$Symbol)
sce.qc.10x <- quickRnaQc.se(sce.10x, subsets=list(MT=is.mito.10x))
sce.qc.10x <- sce.qc.10x[,sce.qc.10x$keep]

# Computing log-normalized expression values.
sce.norm.10x <- normalizeRnaCounts.se(sce.qc.10x, size.factors=sce.qc.10x$sum)
sce.norm.10x
```

We compute the variance of the log-normalized expression values for each gene across all cells [@lun2016step].
We then fit a trend to the variances with respect to the mean (Figure \@ref(fig:trend-plot-pbmc)).
HVGs are identified as the top $H$ genes with the largest residuals above the trend, where $H$ is typically between 1000 and 5000.
We use the variances in the log-values to ensure that the feature selection is based on the same expression values that are used for later downstream steps.
Genes with the largest variances will contribute most to the distances between cells during procedures like clustering and dimensionality reduction.

```{r trend-plot-pbmc, fig.cap="Variance of the log-normalized expression values across all genes in the PBMC data set, as a function of the mean. Each point represents a gene, colored according to whether it was chosen as a HVG. The blue line represents the trend fitted to all genes."}
sce.var.10x <- chooseRnaHvgs.se(sce.norm.10x)

# Let's have a peek at the statistics for the top HVGs.
rd.10x <- rowData(sce.var.10x)
ordered.residual.10x <- order(rd.10x$residuals, decreasing=TRUE)
rd.10x[head(ordered.residual.10x),c("Symbol", "means", "variances", "fitted", "residuals", "hvg")]

# Look at the number of top HVGs.
is.hvg.10x <- rowData(sce.var.10x)$hvg
sum(is.hvg.10x)

plot(rd.10x$means, rd.10x$variances,
    col=ifelse(is.hvg.10x, "black", "grey"), pch=16, cex=1,
    xlab="Mean of log-expression", ylab="Variance of log-expression")
legend("topright", col=c("black", "grey"), pch=16, legend=c("non-HVG", "HVG"))

trend.10x <- approxfun(rd.10x$means, rd.10x$fitted)
curve(trend.10x, add=TRUE, col="dodgerblue", lwd=2)
```

We use residuals to select HVGs as the log-transformation is not a variance stabilizing transformation.
The assumption is that, at any given abundance, the variation in expression for most genes is driven by uninteresting processes like sampling noise. 
The fitted value of the trend at any given gene's abundance represents a mean-dependent estimate of its uninteresting variation,
while the residuals represent the "interesting" variation for each gene and can be used as the metric for HVG selection.
If we just used the total variance without any trend, the choice of HVGs would be driven more by the gene's abundance than its biological heterogeneity. 

Once we have our top HVGs, we can use them in downstream steps like principal components analysis.
We'll discuss this more in Chapter \@ref(principal-components-analysis), but it is as simple as using only the subset of HVGs in the analysis:

```{r}
sce.pcs.10x <- runPca.se(sce.var.10x, features=is.hvg.10x)
```

### Choosing the number of HVGs

How many HVGs should we use in our downstream analyses, i.e., what is the "best" value of $H$?
A larger set of HVGs will reduce the risk of discarding interesting biological signal by retaining more potentially relevant genes,
at the cost of adding noise from irrelevant genes that might obscure that signal.
It's difficult to determine the optimal trade-off for any given application as the distinction between noise and signal is context-dependent.
For example, variation in the activation status of certain immune cells may not be interesting when we are only interested in identifying the cell types;
the former can even interfere with the latter by encouraging the formation of clusters based on activation strength instead.

Our recommendation is to simply pick a reasonable $H$ - usually somewhere between 1000 and 5000 - and proceed with the rest of the analysis.
If we can answer our scientific question, then our choice is good enough; if not, we can just try another value.
There's nothing wrong with trying different parameters during data exploration^[
If would be another story if we were doing some confirmatory analysis with rigorous hypothesis testing.
In such cases, it would be improper to shop around for the best parameters that gives us the result that we want.
Fortunately, single-cell analyses have looser standards as we often don't know what we're looking for.]
In fact, different choices of $H$ can provide new perspectives of the same dataset by changing the balance between signal and noise,
so we might discover new population structure that would not be apparent with other parameters.
Don't spend too much time worrying about obtaining the "optimal" value.

If we really want to ensure that all biological structure is preserved, we could define the set of HVGs as all genes with variances above the trend.
This avoids any judgement calls about the definition of "interesting" variation, giving an opportunity for weaker population structure to manifest.
It is most useful for rare and/or weakly-separated subpopulations where the relevant marker genes are not variable enough to sneak into the top $H$ genes.
The obvious cost is that more noise is also captured, which can reduce the resolution of subpopulations;
and we need to perform more computational work in each downstream step, as more genes are involved.

```{r}
# Setting top=Inf to select all genes with positive residuals.
hvgs.all.10x <- chooseHighlyVariableGenes(rd.10x$residuals, top=Inf)
length(hvgs.all.10x)

# This can be used just like our other HVGs.
sce.all.10x <- runPca.se(sce.var.10x, features=hvgs.all.10x)
```

## Handling multiple batches {#variance-batch}

Larger datasets may contain multiple batches that exhibit uninteresting differences in gene expression, i.e., batch effects.
We are not interested in HVGs that are driven by batch effects; instead, we want to focus on genes that are highly variable within each batch.
We demonstrate using some trophoblast scRNA-seq data generated across two batches [@lun2017assessing]:

```{r}
library(scRNAseq)
sce.tropho <- LunSpikeInData("tropho")
table(sce.tropho$block) # plate of origin, which is our batch.

# Computing the QC metrics.
library(scrapper)
is.mito.tropho <- which(any(seqnames(rowRanges(sce.tropho))=="MT"))
sce.qc.tropho <- quickRnaQc.se(
    sce.tropho,
    subsets=list(MT=is.mito.tropho),
    altexp.proportions="ERCC",
    block=sce.tropho$block
)
sce.qc.tropho <- sce.qc.tropho[,sce.qc.tropho$keep]

# Computing log-normalized expression values.
sce.norm.tropho <- normalizeRnaCounts.se(
    sce.qc.tropho,
    size.factors=sce.qc.tropho$sum,
    block=sce.qc.tropho$block
)
```

Setting `block=` instructs `chooseRnaHvgs.se()` to compute the mean and variance for each gene within each batch.
This ensures that any differences between batches will not inflate the variance estimates.
It will also fit a separate trend for each batch, which accommodates differences in the mean-variance relationships between batches.
This is especially important if batches exhibit systematic technical differences, e.g., differences in coverage.
In this case, there are only minor differences between the trends in Figure \@ref(fig:trend-plot-tropho), which indicates that the experiment was tightly replicated across plates.

```{r trend-plot-tropho, fig.width=10, fig.asp=0.5, fig.cap="Variance of the log-normalized expression values across all genes in the trophoblast data set, as a function of the mean after blocking on the plate of origin. Each plot represents the results for a single plate. Each point represents a gene and the fitted trend is shown in blue."}
sce.var.tropho <- chooseRnaHvgs.se(
    sce.norm.tropho,
    block=sce.qc.tropho$block,
    include.per.block=TRUE # only needed for plotting.
)

per.block <- rowData(sce.var.tropho)$per.block
par(mfrow=c(1,2))
for (block in colnames(per.block)) {
    current <- per.block[[block]]
    plot(current$means, current$variances,
        xlab="Mean of log-expression",
        ylab="Variance of log-expression",
        main=block, pch=16, cex=0.5)
    trend <- approxfun(current$means, current$fitted)
    curve(trend, add=TRUE, col="dodgerblue", lwd=2)
}
```

`modelGeneVariances()` will report the average statistics across batches for feature selection.
This ensures that each batch contributes some information about the variability of each gene. 
We can then use the average residuals to select our top HVGs, as shown below.
An alternative approach is to take the intersection or union of the HVGs from each block, but this does not scale well,
i.e., it becomes too stringent or too relaxed, respectively, as the number of blocks increases.

```{r}
hvgs.tropho <- rowData(sce.var.tropho)$hvg
sum(hvgs.tropho)
```

It is generally expected that `block=` will be used for uninteresting factors of variation.
In this case, the plate of origin is a technical factor that should be ignored. 
However, imagine instead that each plate corresponds to a different treatment condition.
In such cases, we might not use `block=` to ensure that our variance estimates can capture the differences between treatments.
We discuss these considerations in more detail in Chapter \@ref(batch-correction).

As an aside, the wave-like shape observed in Figure \@ref(fig:trend-plot-tropho) is typical of the mean-variance trend for log-expression values.
A linear increase in the variance is observed as the mean increases from zero, as larger variances are obviously possible when the counts are not all equal to zero.
In contrast, the relative contribution of sampling noise decreases at high abundances, resulting in a downward trend.
The peak represents the point at which these two competing effects cancel each other out.

## Refining the trend fit

The trend fit in `modelGeneVariances()` uses the LOWESS non-parametric smoother [@cleveland1979robust] by default.
This slides a window across the x-coordinates and performs a linear regression within each window to obtain the fitted value for the point at the window's center.
The size of the window varies between points, expanding or contracting until it contains some proportion of all points in the dataset (by default, 30%).
This mostly works well but can be suboptimal in very sparse intervals of the x-axis.
To demonstrate, let's have a look at a human pancreas dataset from @segerstolpe2016singlecell:

```{r}
library(scRNAseq)
sce.seger <- SegerstolpePancreasData()

# For simplicity, we'll focus on one of the donors. 
sce.seger <- sce.seger[,sce.seger$individual=="H2"]

# For reasons unknown to us, the data supplied by the authors contain
# duplicated row names, so we'll just get rid of those to avoid confusion.
sce.seger <- sce.seger[!duplicated(rownames(sce.seger)),]

# Running QC. Seems like they don't have any data for the mitochondrial genes,
# unfortunately, but they do have spike-ins so we'll just use those instead.
library(scrapper)
sce.qc.seger <- quickRnaQc.se(sce.seger, subsets=list(), altexp.proportions="ERCC")

# Computing log-normalized expression values.
sce.norm.seger <- normalizeRnaCounts.se(sce.qc.seger, size.factors=sce.qc.seger$sum)
```

There are very few genes at high abundances, which forces the LOWESS window to expand to contain more points.
This reduces sensitivity of the fitted trend to the behavior of the high-abundance genes (Figure \@ref(fig:trend-plot-seger)).
We can improve the fit by tinkering with some of the trend fitting options.
In particular, setting `use.min.width=TRUE` will switch to a different strategy to defining the window around each point,
which improves sensitivity in sparse intervals at the risk of overfitting.

```{r trend-plot-seger, fig.cap="Variance of the log-normalized expression values across all genes in one donor of the Segerstople pancreas data set, as a function of the mean. Each point represents a gene while the lines represent trends fitted with different parameters."}
sce.default.seger <- chooseRnaHvgs.se(sce.norm.seger)
sce.minw.seger <- chooseRnaHvgs.se(sce.norm.seger, more.var.args=list(use.min.width=TRUE))

rd.default.seger <- rowData(sce.default.seger)
plot(rd.default.seger$means, rd.default.seger$variances,
    xlab="Mean of log-expression", ylab="Variance of log-expression", pch=16, cex=0.5)
trend.default.seger <- approxfun(rd.default.seger$means, rd.default.seger$fitted)
curve(trend.default.seger, add=TRUE, col="dodgerblue", lwd=2)

rd.minw.seger <- rowData(sce.minw.seger)
trend.minw.seger <- approxfun(rd.minw.seger$means, rd.minw.seger$fitted)
curve(trend.minw.seger, add=TRUE, col="salmon", lwd=2)
legend("topright", lwd=2, col=c("dodgerblue", "salmon"), legend=c("default", "min-width"))
```

In practice, the parametrization of the trend fitting doesn't matter all that much.
There are so few genes in these sparse intervals that whether or not they are chosen as HVGs won't have a major effect on downstream analyses.
But sometimes it's just nice to look at some well-fitted curves.

## Selecting _a priori_ genes of interest

A blunt yet effective feature selection strategy is to use pre-defined sets of interesting genes.
The aim is to focus on specific aspects of biological heterogeneity that may be masked by other factors when using unsupervised methods for HVG selection.
For example, to study transcriptional changes during the earliest stages of cell fate commitment [@messmer2019transcriptional],
we might focus only on lineage markers to avoid interference from variability in other pathways (e.g., cell cycle, metabolism).
Using scRNA-seq data in this manner is conceptually equivalent to a fluorescence activated cell sorting (FACS) experiment,
with the convenience of being able to (re)define the features of interest at any time.
We provide some examples of _a priori_ selection based on MSigDB gene sets [@godec2016compendium] below:

```{r}
library(msigdbr)
c7.sets <- msigdbr(species = "Homo sapiens", category = "C7")
head(unique(c7.sets$gs_name))

# Using the Goldrath sets to distinguish CD8 subtypes
cd8.sets <- c7.sets[grep("GOLDRATH", c7.sets$gs_name),]
cd8.genes <- rownames(sce.10x) %in% cd8.sets$ensembl_gene
summary(cd8.genes)

# Using GSE11924 to distinguish between T helper subtypes
th.sets <- c7.sets[grep("GSE11924", c7.sets$gs_name),]
th.genes <- rownames(sce.10x) %in% th.sets$ensembl_gene
summary(th.genes)

# Using GSE11961 to distinguish between B cell subtypes
b.sets <- c7.sets[grep("GSE11961", c7.sets$gs_name),]
b.genes <- rownames(sce.10x) %in% b.sets$ensembl_gene
summary(b.genes)
```

Don't be ashamed to take advantage of prior biological knowledge during feature selection to address specific hypotheses!
We say this because a common refrain in genomics is that the data analysis should be "unbiased", i.e., free from any biological preconceptions.
Which is fine and all, but such "biases" are already present at every stage, starting with experimental design and ending with the interpretation of the data.
So if we already know what we're looking for, why not make life simpler and just go for it?
Of course, the downside of focusing on pre-defined genes is that it will limit our capacity to detect novel or unexpected aspects of variation.
Thus, this kind of focused analysis should be complementary to (rather than a replacement for) the unsupervised feature selection strategies discussed above.

Alternatively, we can invert this reasoning to remove genes that are unlikely to be of interest prior to downstream analyses. 
This eliminates unwanted variation that could mask relevant biology and interfere with interpretation of the results.
Ribosomal protein genes or mitochondrial genes are common candidates for removal,
especially in situations with varying levels of cell damage within a population.
For immune cell subsets, we might also be inclined to remove immunoglobulin genes and T cell receptor genes 
for which clonal expression introduces (possibly irrelevant) population structure.

```{r}
# Identifying ribosomal proteins:
ribo.discard <- grepl("^RP[SL]\\d+", rowData(sce.10x)$Symbol)
sum(ribo.discard)

# A more curated approach for identifying ribosomal protein genes:
c2.sets <- msigdbr(species = "Homo sapiens", category = "C2")
ribo.set <- c2.sets[c2.sets$gs_name=="KEGG_RIBOSOME",]$ensembl_gene
ribo.discard <- rownames(sce.10x) %in% ribo.set
sum(ribo.discard)

library(AnnotationHub)
edb <- AnnotationHub()[["AH73881"]]
anno <- select(edb, keys=rowData(sce.10x)$ID, keytype="GENEID", 
    columns="TXBIOTYPE")

# Removing immunoglobulin variable chains:
igv.set <- anno$GENEID[anno$TXBIOTYPE %in% c("IG_V_gene", "IG_V_pseudogene")]
igv.discard <- rownames(sce.10x) %in% igv.set
sum(igv.discard)

# Removing TCR variable chains:
tcr.set <- anno$GENEID[anno$TXBIOTYPE %in% c("TR_V_gene", "TR_V_pseudogene")]
tcr.discard <- rownames(sce.10x) %in% tcr.set
sum(tcr.discard)
```

In practice, we tend to err on the side of caution and abstain from preemptive filtering on biological function 
until these genes are demonstrably problematic in downstream analyses.

## Quantifying technical noise

Back in the old days, everyone was all about modelling the gene-wise variability in scRNA-seq data.
Spike-in transcripts were key to this effort as they allowed us to decompose each gene's variance into technical and biological components. 
As spike-ins should not be subject to biological effects, the variance in spike-in expression could be used as an estimate of the technical component. 
Subtracting this from the variance of an endogenous gene at a similar abundance would then yield an estimate of the biological component.
Sadly, those days are gone and people don't care about variance decomposition anymore.
But for old times' sake, we'll demonstrate how to do this with the @zeisel2015brain dataset:

```{r}
library(scRNAseq)
sce.zeisel <- ZeiselBrainData()
is.mito.zeisel <- rowData(sce.zeisel)$featureType=="mito"

# Performing some QC to set up the dataset prior to normalization.
library(scrapper)
sce.qc.zeisel <- quickRnaQc.se(sce.zeisel, subsets=list(MT=is.mito.zeisel), altexp.proportions="ERCC")
sce.qc.zeisel <- sce.qc.zeisel[,sce.qc.zeisel$keep]

# Computing log-normalized expression values. This time, we do it for both the
# endogenous genes and the spike-ins with their respective size factors.
sce.norm.zeisel <- normalizeRnaCounts.se(sce.qc.zeisel, size.factors=sce.qc.zeisel$sum)
sce.ercc <- altExp(sce.norm.zeisel, "ERCC")
sce.norm.ercc <- normalizeRnaCounts.se(sce.ercc, size.factors=sce.ercc$sum)
```

We fit separate mean-dependent trends to the endogenous genes and spike-in transcripts (Figure \@ref(fig:trend-plot-spike-in)).
At any given mean, the fitted value of the spike-in trend represents an estimate of the techical component of the variance.
This assumes that an endogenous gene is subject to the same technical noise as a spike-in transcript of the same abundance.
Also note that we use two different sets of size factors - the spike-in factors for the spike-in transcripts and the library size factors for the endogenous genes - 
as we are not interested in changes in total RNA content.
We ensure that means are still comparable between genes and spike-ins by centering both sets of size factors to preserve the scale of the original counts. 
(This is a bit more complicated with blocking, as the mean of the spike-in factors within each block must be scaled to the mean of the library size factors in that block;
for brevity, we won't show that here.)

```{r trend-plot-spike-in, fig.cap="Variance of endogenous genes and spike-in transcripts in the Zeisel brain dataset, as a function of the mean. Separate trends are fitted to the genes (blue) and spike-ins (red)."}
sce.var.zeisel <- chooseRnaHvgs.se(sce.norm.zeisel, more.var.args=list(use.min.width=TRUE))
sce.var.ercc <- chooseRnaHvgs.se(sce.norm.ercc)

var.zeisel <- rowData(sce.var.zeisel)
plot(var.zeisel$means, var.zeisel$variances,
    xlab="Mean of log-expression", ylab="Variance of log-expression", pch=16, cex=0.5)
trend.gene.zeisel <- approxfun(var.zeisel$means, var.zeisel$fitted)
curve(trend.gene.zeisel, add=TRUE, col="dodgerblue", lwd=2)

var.ercc <- rowData(sce.var.ercc)
points(var.ercc$means, var.ercc$variances, pch=4)
trend.spike.zeisel <- approxfun(var.ercc$means, var.ercc$fitted, rule=2)
curve(trend.spike.zeisel, add=TRUE, col="salmon", lwd=2, lty=2)

legend("topright", c("gene", "spike-in"), pch=c(16, 4))
```

To decompose each gene's variance into the technical and biological components,
we estimate the fitted value of the spike-in trend at that gene's mean and subtract it from the gene's total variance.
We could then use the biological component to select HVGs via `chooseHighlyVariableGenes()`.
In practice, this doesn't provide much benefit over the residuals from the trend fitted to the endogenous genes.

```{r}
tech.var.zeisel <- trend.spike.zeisel(var.zeisel$means)
summary(tech.var.zeisel)

bio.var.zeisel <- var.zeisel$variances - tech.var.zeisel
summary(bio.var.zeisel)
```

## Session information {-}

```{r}
sessionInfo()
```
